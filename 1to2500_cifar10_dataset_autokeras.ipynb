{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "1to2500_cifar10_dataset-autokeras.ipynb",
      "provenance": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/demolakstate/eGAN_addressing_class_imbalance_with_transfer_learning_on_GAN/blob/main/1to2500_cifar10_dataset_autokeras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD4bIjg6qYkH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7aOHFtUW4Kv"
      },
      "source": [
        "## 2 instances of Normal samples - imbalance ratio 1:2500 ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YN_2kXCOiDUB"
      },
      "source": [
        "### Here, we train the discriminator on samples from normal frames ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHvSaXl0Y6n7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f7f006b-74d1-4a27-883b-adc00652f57a"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELynCH1g6iD2"
      },
      "source": [
        "import time, os"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apdqasPQ6iJh",
        "outputId": "04d99b50-acad-4e50-fc4f-090e1b58d465"
      },
      "source": [
        "print(round(time.time()))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1617527681\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_v8eHk57lnc"
      },
      "source": [
        "time_stamp = round(time.time())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpUZtHhJ1DlL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ec2578c-b26c-44bd-f294-621abf31cfd2"
      },
      "source": [
        "cd /content/gdrive/MyDrive/Anomaly_Detection_in_Images/"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/MyDrive/Anomaly_Detection_in_Images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhLsERzw7TPa"
      },
      "source": [
        "os.mkdir(str(time_stamp))\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVicr1ug85Yp"
      },
      "source": [
        "os.chdir(str(time_stamp))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "665wj8vx76Nq",
        "outputId": "37bd0f5f-a7f1-4acd-d556-257d8fe5329f"
      },
      "source": [
        "time_stamp"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1617527681"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A5KLSqY76RH"
      },
      "source": [
        ""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CPoIbnzQqTXC",
        "outputId": "12594d4f-7921-4455-c382-f31a9814602d"
      },
      "source": [
        "#!pip install -q tensorflow-gpu==2.0.0-alpha0\n",
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUdnxrdkqTXE"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw_b6GnEp1vK"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8Po8E1bp196"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDQHjRVIqTXF"
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "# \"GPU\" + device_lib.list_local_devices()[-1].physical_device_desc.split(\",\")[1]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "R5ASv7D0qTXG",
        "outputId": "adb023ae-f483-4cc0-87f8-8970b0b30d4e"
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzjXZHG4qTXG"
      },
      "source": [
        "import glob # The glob module finds all the pathnames matching a specified pattern according to the rules used by the Unix shell, although results are returned in arbitrary order. \n",
        "import imageio # Imageio is a Python library that provides an easy interface to read and write a wide range of image data, including animated images, volumetric data, and scientific formats. \n",
        "%matplotlib inline \n",
        "import matplotlib.pyplot as plt \n",
        "import tensorflow.keras.layers as layers # Keras layers API\n",
        "import time\n",
        "from IPython import display # For displaying image"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN1HuYsUs_JS"
      },
      "source": [
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "id": "ZiFlOzTtqTXG"
      },
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the \"../input/\" directory.\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
        "import os\n",
        "import PIL\n",
        "import random"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72JdEgtVr59A"
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "maJ4mfVa2Bqh"
      },
      "source": [
        "#!pip install -q -U tensorboard\n",
        "!pip install autokeras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAp3I2bvE7QO"
      },
      "source": [
        "from autokeras import ImageClassifier\n",
        "from tensorflow import keras"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAJDx8Pu5jKv"
      },
      "source": [
        "## Load dataset ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwYwvVXM5lvY"
      },
      "source": [
        "# example of loading the cifar10 dataset\n",
        "from keras.datasets.cifar10 import load_data\n",
        "from matplotlib import pyplot\n",
        "# load the images into memory\n",
        "(X_train, X_train_label), (X_test, X_test_label) = load_data()\n",
        "X_train_imbalance = []\n",
        "#X_train_normal = []\n",
        "\n",
        "X_train_label_imbalance = []\n",
        "# y_train_normal = []\n",
        "\n",
        "count = 1\n",
        "\n",
        "# plot images from the training dataset\n",
        "for i in range(len(X_train)):\n",
        "  if X_train_label[i] == 0 and count <= 2: # few abnormal samples\n",
        "    X_train_imbalance.append(X_train[i])\n",
        "    X_train_label_imbalance.append([0])\n",
        "    count += 1\n",
        "  elif X_train_label[i] == 1:\n",
        "    X_train_imbalance.append(X_train[i])\n",
        "    X_train_label_imbalance.append([1])"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrfEhCYMDMPl"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21vsEXt6LleN"
      },
      "source": [
        "# validation data\n",
        "X_val_balance = []\n",
        "#X_train_normal = []\n",
        "\n",
        "X_val_label_balance = []\n",
        "# y_train_normal = []\n",
        "\n",
        "\n",
        "# plot images from the training dataset\n",
        "for i in range(len(X_test)):\n",
        "  if X_test_label[i] == 0: # few abnormal samples\n",
        "    X_val_balance.append(X_test[i])\n",
        "    X_val_label_balance.append([0])\n",
        "    count += 1\n",
        "  elif X_train_label[i] == 1:\n",
        "    X_val_balance.append(X_test[i])\n",
        "    X_val_label_balance.append([1])"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjvv9FPOLliQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3pUgDksMQkA"
      },
      "source": [
        "## Normalize the images to the range [0, 1] ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j1T2GaFa5lzX"
      },
      "source": [
        "X_train_imbalance = np.asarray(X_train_imbalance) \n",
        "#X_train_abnormal = np.asarray(X_train_abnormal[:2]) # use first 2 samples"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKzJ_SdJMJv0"
      },
      "source": [
        "X_val_balance = np.asarray(X_val_balance)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S9jDr8qTMhvQ"
      },
      "source": [
        "X_val_label_balance_np = np.asarray(X_val_label_balance)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmNb2CxTYMsv",
        "outputId": "885679fa-ae85-45f0-bbab-293f89080ee0"
      },
      "source": [
        "len(X_train_imbalance)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5002"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q31rPvRRYMwn"
      },
      "source": [
        "#len(X_train_abnormal)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ_72NAkFTjy"
      },
      "source": [
        "X_train_label_imbalance_np = np.asarray(X_train_label_imbalance)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFaS7Dr7GPAx",
        "outputId": "e0c2608e-8a57-4a27-9cf0-b2b0474e0c7e"
      },
      "source": [
        "X_train[0]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[ 59,  62,  63],\n",
              "        [ 43,  46,  45],\n",
              "        [ 50,  48,  43],\n",
              "        ...,\n",
              "        [158, 132, 108],\n",
              "        [152, 125, 102],\n",
              "        [148, 124, 103]],\n",
              "\n",
              "       [[ 16,  20,  20],\n",
              "        [  0,   0,   0],\n",
              "        [ 18,   8,   0],\n",
              "        ...,\n",
              "        [123,  88,  55],\n",
              "        [119,  83,  50],\n",
              "        [122,  87,  57]],\n",
              "\n",
              "       [[ 25,  24,  21],\n",
              "        [ 16,   7,   0],\n",
              "        [ 49,  27,   8],\n",
              "        ...,\n",
              "        [118,  84,  50],\n",
              "        [120,  84,  50],\n",
              "        [109,  73,  42]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[208, 170,  96],\n",
              "        [201, 153,  34],\n",
              "        [198, 161,  26],\n",
              "        ...,\n",
              "        [160, 133,  70],\n",
              "        [ 56,  31,   7],\n",
              "        [ 53,  34,  20]],\n",
              "\n",
              "       [[180, 139,  96],\n",
              "        [173, 123,  42],\n",
              "        [186, 144,  30],\n",
              "        ...,\n",
              "        [184, 148,  94],\n",
              "        [ 97,  62,  34],\n",
              "        [ 83,  53,  34]],\n",
              "\n",
              "       [[177, 144, 116],\n",
              "        [168, 129,  94],\n",
              "        [179, 142,  87],\n",
              "        ...,\n",
              "        [216, 184, 140],\n",
              "        [151, 118,  84],\n",
              "        [123,  92,  72]]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iY5O1rxZGWE_",
        "outputId": "72a22654-f2e5-4150-ecc2-3507ed66abb5"
      },
      "source": [
        "X_train_imbalance[0]"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[170, 180, 198],\n",
              "        [168, 178, 196],\n",
              "        [177, 185, 203],\n",
              "        ...,\n",
              "        [162, 179, 215],\n",
              "        [158, 178, 214],\n",
              "        [157, 177, 212]],\n",
              "\n",
              "       [[168, 181, 198],\n",
              "        [172, 185, 201],\n",
              "        [171, 183, 200],\n",
              "        ...,\n",
              "        [159, 177, 212],\n",
              "        [156, 176, 211],\n",
              "        [154, 174, 209]],\n",
              "\n",
              "       [[154, 170, 186],\n",
              "        [149, 165, 181],\n",
              "        [129, 144, 162],\n",
              "        ...,\n",
              "        [161, 178, 214],\n",
              "        [157, 177, 212],\n",
              "        [154, 174, 209]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[ 74,  84,  80],\n",
              "        [ 76,  85,  81],\n",
              "        [ 78,  85,  82],\n",
              "        ...,\n",
              "        [ 71,  75,  78],\n",
              "        [ 68,  72,  75],\n",
              "        [ 61,  65,  68]],\n",
              "\n",
              "       [[ 68,  76,  77],\n",
              "        [ 69,  77,  78],\n",
              "        [ 72,  79,  78],\n",
              "        ...,\n",
              "        [ 76,  80,  83],\n",
              "        [ 71,  75,  78],\n",
              "        [ 71,  75,  78]],\n",
              "\n",
              "       [[ 67,  75,  78],\n",
              "        [ 68,  76,  79],\n",
              "        [ 69,  75,  76],\n",
              "        ...,\n",
              "        [ 75,  79,  82],\n",
              "        [ 71,  75,  78],\n",
              "        [ 73,  77,  80]]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx9bXOjhKMza"
      },
      "source": [
        ""
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prfdAec6KNDs"
      },
      "source": [
        "X_train_imbalance = X_train_imbalance.astype('float32') / 255.0"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b_w0PBS0MypA"
      },
      "source": [
        "X_val_balance = X_val_balance.astype('float32') / 255.0"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dL0VIE0_FBMy"
      },
      "source": [
        "## Define the model ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q65sdBj0Et58"
      },
      "source": [
        "METRICS = [\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsFPqrCMEt-X"
      },
      "source": [
        "EPOCHS = 2"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XF0BofcIFH9g"
      },
      "source": [
        "classifier = ImageClassifier(seed=9, max_trials=2, metrics=METRICS)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bKortwQFH_D",
        "outputId": "126ba548-c031-4118-f9c2-3135dbb0f75d"
      },
      "source": [
        "classifier.fit(X_train_imbalance, X_train_label_imbalance_np, epochs=EPOCHS)\t"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Trial 2 Complete [00h 01m 03s]\n",
            "val_loss: 8.99029600986978e-07\n",
            "\n",
            "Best val_loss So Far: 1.1079739209440831e-10\n",
            "Total elapsed time: 00h 01m 41s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Epoch 1/2\n",
            "157/157 [==============================] - 3s 9ms/step - loss: 0.0304 - tp: 3499.5190 - fp: 2.0000 - tn: 0.0000e+00 - fn: 12.0000 - accuracy: 0.9950 - precision: 0.9993 - recall: 0.9957 - auc: 0.9916\n",
            "Epoch 2/2\n",
            "157/157 [==============================] - 1s 9ms/step - loss: 0.0727 - tp: 2541.5190 - fp: 2.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - accuracy: 0.9978 - precision: 0.9978 - recall: 1.0000 - auc: 0.4662\n",
            "INFO:tensorflow:Assets written to: ./image_classifier/best_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znEDQIbOFIDg",
        "outputId": "f434beda-f414-4a71-dc51-3543c586f226"
      },
      "source": [
        "print(classifier.evaluate(X_val_balance, X_val_label_balance_np))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59/59 [==============================] - 2s 7ms/step - loss: 8.9154 - tp: 876.0000 - fp: 1000.0000 - tn: 0.0000e+00 - fn: 0.0000e+00 - accuracy: 0.4670 - precision: 0.4670 - recall: 1.0000 - auc: 0.5000\n",
            "[8.915423393249512, 876.0, 1000.0, 0.0, 0.0, 0.4669509530067444, 0.4669509530067444, 1.0, 0.5]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R--vPpDlNNYA",
        "outputId": "006bf4ad-57a4-4688-bae9-5938aa07bb7b"
      },
      "source": [
        "len(X_val_balance)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1876"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giCMM2Z4NNhV",
        "outputId": "dcbb2ebf-352f-484b-d818-cc98dc52e500"
      },
      "source": [
        "len(X_val_label_balance_np)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1876"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxTveuFKNRTP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D93TTQBvNRXP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuCDdJ01FIHX",
        "outputId": "896df279-b65c-4667-d253-bd282d3f02cc"
      },
      "source": [
        "X_test_label"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3],\n",
              "       [8],\n",
              "       [8],\n",
              "       ...,\n",
              "       [5],\n",
              "       [1],\n",
              "       [7]], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh0M_jbYNMoB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nsS-rYy7FTnq",
        "outputId": "33ac0e46-5145-4c7f-f5c6-551724fbf116"
      },
      "source": [
        "X_train_label_imbalance_np"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1],\n",
              "       [1],\n",
              "       [0],\n",
              "       ...,\n",
              "       [1],\n",
              "       [1],\n",
              "       [1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1M0Y-l5xFTrd"
      },
      "source": [
        ""
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "id": "82Z8RjegEuCS",
        "outputId": "3477829c-1edb-4c7c-cf09-d600e56754bf"
      },
      "source": [
        "---"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-33-29e0c3615294>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    ---\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJZFN0ZyEuHF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8coq_DioOfW"
      },
      "source": [
        "abnormal_data = X_train_abnormal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FK4KbQEWTXj"
      },
      "source": [
        "train_images = X_train_normal.reshape(X_train_normal.shape[0], 32, 32, 3).astype('float32')\n",
        "#train_images = (train_images - 127.5) / 127.5\n",
        "\n",
        "train_images = (train_images) / 255.0\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9nWGeHknZqWi"
      },
      "source": [
        "X_train_normal.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYwdQfEKZqbY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol3vSNgFDhpl"
      },
      "source": [
        "abnormal_data = abnormal_data.astype('float32')\n",
        "#normal_data = normal_data.astype('float32')\n",
        "\n",
        "abnormal_data = abnormal_data / 255.0\n",
        "#normal_data = normal_data / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5-oEA2bd9IC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSV0pNbeOv3M"
      },
      "source": [
        "X_test_abnormal = []\n",
        "X_test_normal = []\n",
        "\n",
        "# plot images from the training dataset\n",
        "for i in range(len(X_test)):\n",
        "  if X_test_label[i] == 0: # few abnormal samples\n",
        "    X_test_abnormal.append(X_test[i])\n",
        "  elif X_test_label[i] == 1:\n",
        "    X_test_normal.append(X_test[i])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ukju9aFz79as"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7w2zz27q79rY"
      },
      "source": [
        "X_validation_abnormal = X_test_abnormal[:500] # use first 500 as abnormal (validation)\n",
        "X_validation_normal = X_test_normal[:500] # use first 500 as normal (validation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZumYSoOR_kId"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29U6XilX_kTG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IvGp4YLROqa"
      },
      "source": [
        "# expand to 3d, e.g. add channels dimension\n",
        "abnormal_data_validation = np.expand_dims(X_validation_abnormal, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JkwO2RDdOwDv"
      },
      "source": [
        "abnormal_data_validation = abnormal_data_validation.astype('float32')\n",
        "abnormal_data_validation = abnormal_data_validation / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4H4yhdJPYok"
      },
      "source": [
        "#abnormal_images = np.array(os.listdir(path_abnormal))\n",
        "np.random.shuffle(abnormal_data_validation)\n",
        "BUFFER_SIZE = len(X_validation_abnormal) #200000 # number of images in training i think\n",
        "BATCH_SIZE = 1#500 # This is just the standard number for batch size. Google for more info\n",
        "# shuffle and batch the data\n",
        "np.random.shuffle(abnormal_data_validation)\n",
        "abnormal_data_validation = np.split(abnormal_data_validation[:BUFFER_SIZE],BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPTvr3l8P_-8"
      },
      "source": [
        "X_validation_normal = np.asarray(X_validation_normal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Grufe46QADl"
      },
      "source": [
        "validation_normal_images = X_validation_normal.reshape(X_validation_normal.shape[0], 32, 32, 3).astype('float32')\n",
        "#train_images = (train_images - 127.5) / 127.5\n",
        "\n",
        "validation_normal_images = (validation_normal_images) / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jsx_LALn7zFI"
      },
      "source": [
        "validation_normal_ds = tf.data.Dataset.from_tensor_slices(validation_normal_images).shuffle(54077).batch(100000000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgOVbw095qCd"
      },
      "source": [
        "# X_train_normal_np = X_train_normal_np.astype('float32') / 255.\n",
        "# X_train_abnormal_np = X_train_abnormal_np.astype('float32') / 255."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFD4eRcvqTXH"
      },
      "source": [
        "# path = \"/content/gdrive/MyDrive/Anomaly_Detection_Videos/normal_frames/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4pyk1fpg--V"
      },
      "source": [
        "# path_abnormal = \"/content/gdrive/MyDrive/Anomaly_Detection_Videos/abnormal_frames/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2aq3snQqTXH"
      },
      "source": [
        "# def load_image( infilename ) :\n",
        "#     img = PIL.Image.open( infilename )\n",
        "#     #img = img.crop([25,65,153,193])\n",
        "#     #img = img.resize((64,64))\n",
        "#     img = img.resize((512,512))\n",
        "#     data = np.asarray( img, dtype=\"int32\" )\n",
        "#     return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPu_SL8nscPu"
      },
      "source": [
        "latent_dim = 128\n",
        "C = 3\n",
        "img_width = img_height = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZnINbCQYiCj"
      },
      "source": [
        "IMG_SIZE = (32, 32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyzrcusrYiH0"
      },
      "source": [
        "IMG_SHAPE = IMG_SIZE + (3,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw5bZzws8fdZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfQKg-dhX8fW"
      },
      "source": [
        "base_model_encoder = tf.keras.applications.DenseNet121(input_shape=IMG_SHAPE,\n",
        "                                            include_top=False,\n",
        "                                            weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXuOE3mvX8ke"
      },
      "source": [
        "base_model_encoder.trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zpri_oGgYKUh"
      },
      "source": [
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model_encoder.layers))\n",
        "\n",
        "# Fine-tune from this layer onwards\n",
        "fine_tune_at_encoder = 422\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model_encoder.layers[:fine_tune_at_encoder]:\n",
        "  layer.trainable =  False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DLGyCBlppYCC"
      },
      "source": [
        "print(f'{len(base_model_encoder.layers) - fine_tune_at_encoder} layers fine-tuned at generator module')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAYO2zxqpYJZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCZSULVkYKZL"
      },
      "source": [
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdSPwz6GYKdj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXUrrksy8fhs"
      },
      "source": [
        "def make_encoder_model():\n",
        "  inputs = layers.Input(shape=(img_width, img_height, C), name=\"inputs\")\n",
        "  x = inputs\n",
        "\n",
        "  # Fine-tuning the base layers\n",
        "\n",
        "  x = base_model_encoder(inputs, training=False)\n",
        "  x = global_average_layer(x)\n",
        "  #x = tf.keras.layers.Dropout(0.2)(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # x = layers.Conv2D(32, (3,3), padding=\"same\")(x)\n",
        "  # x = layers.BatchNormalization()(x)\n",
        "  # x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "  # x = layers.MaxPool2D((2,2))(x)\n",
        "\n",
        "  # x = layers.Conv2D(32, (3,3), padding=\"same\")(x)\n",
        "  # x = layers.BatchNormalization()(x)\n",
        "  # x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "  # x = layers.MaxPool2D((2,2))(x)\n",
        "\n",
        "  print('shape after encoder: ', x.shape)\n",
        "\n",
        "  # x = layers.Flatten()(x)\n",
        "  units = x.shape[1] * 2\n",
        "  x = layers.Dense(latent_dim, name=\"latent\")(x)\n",
        "\n",
        "  return (x, inputs, units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNYtNMdF8fl6"
      },
      "source": [
        "def make_generator_model(x, units):\n",
        "  # Building the generator\n",
        "  x = layers.Dense(units)(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "  x = layers.Reshape((8, 8, 32))(x) # reshape to shape after encoder\n",
        "\n",
        "  x = layers.Conv2DTranspose(64, (3, 3), strides=2, padding=\"same\")(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "  x = layers.Conv2DTranspose(C, (3,3), strides=2, padding='same')(x)\n",
        "  x = layers.BatchNormalization()(x)\n",
        "  x = layers.Activation(\"sigmoid\", name=\"outputs\")(x)\n",
        "\n",
        "  outputs = x\n",
        "\n",
        "  return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0XkFJ1aCqB4"
      },
      "source": [
        "x, inputs, units = make_encoder_model()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WlJxGlN8CwBv"
      },
      "source": [
        "outputs = make_generator_model(x=x, units=units)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lpD6HVQCLjP"
      },
      "source": [
        "encoder_generator_network = Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVs7XgajsN5_"
      },
      "source": [
        "print(x.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4bNKAlYV_egO"
      },
      "source": [
        "abnormal_data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhqB68KtCXjK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH1fvSBtKEQG"
      },
      "source": [
        "#abnormal_images = np.array(os.listdir(path_abnormal))\n",
        "np.random.shuffle(abnormal_data)\n",
        "BUFFER_SIZE = len(X_train_abnormal) #200000 # number of images in training i think\n",
        "BATCH_SIZE = 1#500 # This is just the standard number for batch size. Google for more info\n",
        "# shuffle and batch the data\n",
        "np.random.shuffle(abnormal_data)\n",
        "# abnormal_data = np.split(abnormal_data[:BUFFER_SIZE],BATCH_SIZE)\n",
        "\n",
        "abnormal_data = np.split(abnormal_data[:2500], 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-vj7oPxKEVB"
      },
      "source": [
        "# #abnormal_images = np.array(os.listdir(path_abnormal))\n",
        "# np.random.shuffle(normal_data)\n",
        "# BUFFER_SIZE = 5#200000 # number of images in training i think\n",
        "# BATCH_SIZE = 1#500 # This is just the standard number for batch size. Google for more info\n",
        "# # shuffle and batch the data\n",
        "# np.random.shuffle(normal_data)\n",
        "# #normal_data = np.split(normal_data[:BUFFER_SIZE],BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh-o7AjMV0yF"
      },
      "source": [
        "train_normal_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(54077).batch(128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COO4SI8nCXng"
      },
      "source": [
        "#len(abnormal_data[0][1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrAL-SrYCXri"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWO5D3VhqTXJ"
      },
      "source": [
        "#generator = make_generator_model()\n",
        "\n",
        "#noise = tf.random.normal([1, img_width,img_height, 1]) # shape is 1, 100\n",
        "generated_image = encoder_generator_network(abnormal_data, training = True)\n",
        "#plt.imshow(generated_image[0], interpolation=\"nearest\" )\n",
        "plt.imshow(generated_image[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIS1LB3fEKge"
      },
      "source": [
        "len(generated_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_JX0ygx1xSM"
      },
      "source": [
        "IMG_SIZE = (32, 32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7BC5Ze0140V"
      },
      "source": [
        "IMG_SHAPE = IMG_SIZE + (3,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLw6Wcgl1YZF"
      },
      "source": [
        "base_model_disc = tf.keras.applications.DenseNet121(input_shape=IMG_SHAPE,\n",
        "                                            include_top=False,\n",
        "                                            weights='imagenet')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4CDCFe_I2X6V"
      },
      "source": [
        "base_model_disc.trainable = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOq_9yDAMpt8"
      },
      "source": [
        "#base_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0Lq2PKSMqr8"
      },
      "source": [
        "# Let's take a look to see how many layers are in the base model\n",
        "print(\"Number of layers in the base model: \", len(base_model_disc.layers))\n",
        "\n",
        "# Fine-tune from this layer onwards\n",
        "fine_tune_at_disc = 422\n",
        "\n",
        "# Freeze all the layers before the `fine_tune_at` layer\n",
        "for layer in base_model_disc.layers[:fine_tune_at_disc]:\n",
        "  layer.trainable =  False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FyOq3W320La"
      },
      "source": [
        "print(f'{len(base_model_disc.layers) - fine_tune_at_disc} layers fine-tuned at discriminator module')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0j63xQlYp0_6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gOUWK3mc4ASy"
      },
      "source": [
        "prediction_layer = tf.keras.layers.Dense(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZIr4xHs4AW-"
      },
      "source": [
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGkPavlnAVte"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhC_gomoZhwp"
      },
      "source": [
        "# example of defining the discriminator model\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Conv2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.utils.vis_utils import plot_model\n",
        "\n",
        "\n",
        "def define_discriminator():\n",
        "  inputs = tf.keras.Input(shape=(32, 32, 3))\n",
        "  #x = data_augmentation(inputs)\n",
        "  #x = preprocess_input(x)\n",
        "  x = base_model_disc(inputs, training=False)\n",
        "  x = global_average_layer(x)\n",
        "  x = tf.keras.layers.Dropout(0.2)(x)\n",
        "  outputs = prediction_layer(x)\n",
        "  model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "  return model\n",
        "\n",
        "\n",
        " \n",
        "# define the standalone discriminator model\n",
        "def define_discriminator2(in_shape=(img_width, img_height, C)):\n",
        " model = Sequential()\n",
        " model.add(Conv2D(64, (5,5), strides=(2, 2), padding='same', input_shape=in_shape))\n",
        " model.add(LeakyReLU(alpha=0.2))\n",
        " model.add(Dropout(0.4))\n",
        " model.add(Conv2D(128, (5,5), strides=(2, 2), padding='same'))\n",
        " model.add(LeakyReLU(alpha=0.2))\n",
        " model.add(Dropout(0.4))\n",
        " model.add(Flatten())\n",
        " model.add(Dense(1, activation='sigmoid'))\n",
        " # compile model\n",
        " #opt = Adam(lr=0.0002, beta_1=0.5)\n",
        " #model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
        " return model\n",
        " \n",
        "# define discriminator model\n",
        "discriminator = define_discriminator()\n",
        "# summarize the model\n",
        "discriminator.summary()\n",
        "# plot the model\n",
        "plot_model(discriminator, to_file='discriminator_plot.png', show_shapes=True, show_layer_names=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1lRmOOsbGh3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHdpzCmbbGmF"
      },
      "source": [
        "decision = discriminator(generated_image)\n",
        "print(decision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DB5sg7RXEZYj"
      },
      "source": [
        "len(decision)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qs_S3MejNCHX"
      },
      "source": [
        "## Testing Abnormal data with discriminator ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dlujBYpF-fW"
      },
      "source": [
        "abn_data = np.expand_dims(abnormal_data[0][-1], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2S2qAFqlEvzm"
      },
      "source": [
        "decision = discriminator(abn_data)\n",
        "print(decision) # discriminator decision should be close to 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8Wo5Nf5wCva"
      },
      "source": [
        "type(abn_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28CZSdQzbGu3"
      },
      "source": [
        "plt.imshow(tf.squeeze(abn_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S4DCvenAKh0x"
      },
      "source": [
        "train_images[0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jwJpR577MX0z"
      },
      "source": [
        "## Testing Normal data with discriminator ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN_M5tHIKA2V"
      },
      "source": [
        "nor_data = np.expand_dims(train_images[-1], axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JPd_QFwnKA6P"
      },
      "source": [
        "decision = discriminator(nor_data)\n",
        "print(decision) # discriminator decision should be close to 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UInXhBIZKA-f"
      },
      "source": [
        "plt.imshow(tf.squeeze(nor_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhgWJhlpGzYI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGgpbeW2qTXK"
      },
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n",
        "\n",
        "\"\"\"\n",
        "Discriminator Loss\n",
        "\n",
        "This method quantifies how well the discriminator is able to distinguish real images from fakes. It compares the discriminator's predicitions on real images to an array of 1s\n",
        "and the dicriminator's predicitons on fake (generated) images to an array of 0s.\n",
        "\"\"\"\n",
        "@tf.function\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "  real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "  fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "\n",
        "  #abnormal_loss = cross_entropy(tf.zeros_like(abnormal_output), abnormal_output)\n",
        "\n",
        "  #total_loss = real_loss + fake_loss + abnormal_loss\n",
        "  total_loss = real_loss + fake_loss\n",
        "  \n",
        "  return total_loss\n",
        "\n",
        "\"\"\"\n",
        "Generator Loss\n",
        "\n",
        "The generator's loss quantifies how well it was able to trick the discrimator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1).\n",
        "Here, we will compare the discriminators decisions on the generated images to an array of 1s.\n",
        "\"\"\"\n",
        "@tf.function\n",
        "def generator_loss(fake_output):\n",
        "  # pick abnormal image at random for optimization\n",
        "  i = random.randint(0, len(abnormal_data))\n",
        "  print(f'i = {i}')\n",
        "\n",
        "  loss_1 = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "  #loss_2 = cross_entropy(tf.ones_like(abnormal_data[0][i]), abnormal_data[0][i])\n",
        "  #loss_2 = cross_entropy(tf.ones_like(generated_images[i]), abnormal_data[0][i])\n",
        "  #loss_2 = cross_entropy(tf.zeros_like(abnormal_output), abnormal_output)\n",
        "  total_loss = loss_1\n",
        "  return total_loss\n",
        "\n",
        "\"\"\"\n",
        "The discriminator and the generator optimizers are different since we will train two networks separately.\n",
        "The Adam optimization algorithm is an extension to stochastic gradient descent.\n",
        "Stochastic gradient descent maintains a single learning rate (termed alpha) for all weight updates and the learning rate does not change during training.\n",
        "A learning rate is maintained for each network weight (parameter) and separately adapted as learning unfolds.\n",
        "\n",
        "\"\"\"\n",
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XuWcKeCqTXL"
      },
      "source": [
        "checkpoint_dir = \"./training_checkpoints\"\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer = generator_optimizer,\n",
        "                                 discriminator_optimizer = discriminator_optimizer,\n",
        "                                 generator = encoder_generator_network,\n",
        "                                 discriminator = discriminator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6JEwKwVqTXM"
      },
      "source": [
        "checkpoint_prefix"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WQJ798IqTXM"
      },
      "source": [
        "# We will reuse this seed overtime (so it's easier) to visualize progress in the animated GIF\n",
        "#tf.random.set_seed(1234)\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "#seed = tf.random.normal([num_examples_to_generate, noise_dim], seed=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbdY3pjYqTXM"
      },
      "source": [
        "EPOCHS = 4000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSs13_kU3z8-"
      },
      "source": [
        "import datetime\n",
        "log_dir=\"logs/\"\n",
        "\n",
        "summary_writer = tf.summary.create_file_writer(\n",
        "  log_dir + \"fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9rv9o_yqTXM"
      },
      "source": [
        "\"\"\"\n",
        "The training loop begins with generator receiving a random seed as input. \n",
        "That seed is used to produce an image. The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator). \n",
        "The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator.\n",
        "\"\"\"\n",
        "\n",
        "# Notice the use of tf.function\n",
        "# This annotation causes the function to be \"compiled\"\n",
        "def train_step(images, epoch):\n",
        "    #noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "    \n",
        "    # take the image links and return a cropped image\n",
        "    # new_images = []\n",
        "    # for file_name in images:\n",
        "    #     #new_pic = load_image( path + file_name)\n",
        "    #     #new_images.append(new_pic)\n",
        "    #     new_images.append(file_name)\n",
        "    \n",
        "    # images = np.array(new_images)\n",
        "    #images = images.reshape(images.shape[0], img_width, img_height, C).astype('float32') # puts each number in its own numpy array so instead of [1,2,3] gonna be [[1], [2], [3]]\n",
        "    #images = (images) / 255 # normalize to [0,1]\n",
        "    \n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "GradientTape() Records operations for automatic differentiation. Operations are recorded if \n",
        "they are executed within this context manager and at least one of their inputs is being \"watched\".\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = encoder_generator_network(abnormal_data, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True) # images constist of only normal samples\n",
        "      #abnormal_output = discriminator(abnormal_data, training=True) # abnormal_data constist of only abnormal samples\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      #gen_loss = generator_loss(generated_images=generated_images, fake_output=fake_output)\n",
        "      gen_loss = generator_loss(fake_output=fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "    \n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, encoder_generator_network.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, encoder_generator_network.trainable_variables)) # The zip() function returns an iterator of tuples based on the iterable object.\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "    \n",
        "    images = None\n",
        "\n",
        "    with summary_writer.as_default():\n",
        "      tf.summary.scalar('gen_loss', gen_loss, step=epoch)\n",
        "      tf.summary.scalar('disc_loss', disc_loss, step=epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGKJlanCqTXN"
      },
      "source": [
        "# ended at 20 epocsh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5kQUhUCgQHZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-agqOOiHgQL0"
      },
      "source": [
        "  # To plot model performance at various epochs on abnormal and normal samples\n",
        "  chart_abnormal = []\n",
        "  chart_normal = []\n",
        "  chart_epoch = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPpId52ejiJG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4wqVQodTLi5"
      },
      "source": [
        "def discriminator_inference(X_validation_abnormal, X_validation_normal, epoch):\n",
        "  \"\"\"function to calculate discriminator score on text data\"\"\"\n",
        "  print(f'Abnormal-count{\"Normal-count\":>17}')\n",
        "  # print(f'{len(predictions_abnormal_validation[0]):>14}{len(decision_validation_normal[0]):>14}')\n",
        "\n",
        "  # save model to disk at every epoch\n",
        "  discriminator.save(f'{time_stamp}/{epoch}.h5')\n",
        "\n",
        "  predictions_abnormal_validation = []\n",
        "\n",
        "  # # To plot model performance at various epochs on abnormal and normal samples\n",
        "  # chart_abnormal = []\n",
        "  # chart_normal = []\n",
        "  # chart_epoch = []\n",
        "\n",
        "  for index, data in enumerate(abnormal_data_validation[0]):\n",
        "    abn_data_2 = abnormal_data_validation[0][index]\n",
        "    predictions_abnormal_validation.append(discriminator(abn_data_2))\n",
        "\n",
        "\n",
        "  count_validation_abnormal = 0\n",
        "\n",
        "  for ind, _ in enumerate(predictions_abnormal_validation[0]):\n",
        "    if (tf.keras.backend.get_value(predictions_abnormal_validation[0][ind])) < 0: # setting a threshold of 0.9\n",
        "      count_validation_abnormal += 1\n",
        "\n",
        "  print('Abnormal correct count: ', count_validation_abnormal)\n",
        "\n",
        "\n",
        "  decision_validation_normal = []\n",
        "\n",
        "  for index, img in enumerate(validation_normal_ds):\n",
        "    decision = discriminator(img)\n",
        "    decision_validation_normal.append(decision)\n",
        "    #print(decision) # discriminator decision should be close to 1\n",
        "\n",
        "\n",
        "  count_validation_normal = 0\n",
        "\n",
        "  for ind, _ in enumerate(decision_validation_normal[0]):\n",
        "    if (tf.keras.backend.get_value(decision_validation_normal[0][ind]))[0] >= 0: # setting a threshold of 0.9\n",
        "      #print(decision_validation_normal[ind])\n",
        "      count_validation_normal += 1\n",
        "  print('Normal correct count: ', count_validation_normal)\n",
        "\n",
        "\n",
        "  chart_abnormal.append(count_validation_abnormal)\n",
        "  chart_normal.append(count_validation_normal)\n",
        "  chart_epoch.append(epoch)\n",
        "\n",
        "  # return (chart_abnormal, chart_normal, chart_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS4RhZzLgQQe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOw6GeCPqTXN"
      },
      "source": [
        "#@tf.function\n",
        "def train(dataset, epochs):  \n",
        "  tf.print(\"Starting man!\")\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "    tf.print(\"Starting Epoch:\", epoch)\n",
        "    batch_count = 1\n",
        "    for image_batch in dataset:\n",
        "      #tf.print(\"Batch:\", batch_count)\n",
        "      train_step(image_batch, epoch)\n",
        "      #print(\"Batch:\", batch_count, \"Complete\")\n",
        "      batch_count += 1\n",
        "    \n",
        "\n",
        "    # Produce images for the GIF as we go\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(encoder_generator_network,\n",
        "                             epoch + 1,\n",
        "                             abnormal_data)\n",
        "    \n",
        "    tf.print(\"Epoch:\", epoch, \"finished\")\n",
        "    tf.print()\n",
        "    \n",
        "    # Save the model every epochs\n",
        "    #checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "    tf.print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "    if epoch % EPOCHS == 0:\n",
        "       checkpoint.save(file_prefix = checkpoint_prefix)  \n",
        "  # Generate after the final epoch\n",
        "  display.clear_output(wait=True)\n",
        "  generate_and_save_images(encoder_generator_network,\n",
        "                           epochs,\n",
        "                           abnormal_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_SpCmhRqTXN"
      },
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False. \n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(abnormal_data, training=False).numpy()\n",
        "\n",
        "  fig = plt.figure(figsize=(150,150))\n",
        "  print(f'No of predictions: {predictions.shape[0]}')\n",
        "\n",
        "  #plt.imshow(tf.squeeze(predictions[0]), cmap='gray')\n",
        "\n",
        "  discriminator_inference(X_test_abnormal, X_test_normal, epoch)\n",
        "  \n",
        "  for i in range(1):\n",
        "      plt.subplot(1, 1, i+1)\n",
        "      plt.imshow(tf.squeeze(predictions[i]))\n",
        "      #plt.axis('off')\n",
        "\n",
        "      # discriminator_inference(X_test_abnormal, X_test_normal, epoch)\n",
        "      if epoch % 2 == 0:\n",
        "        pass\n",
        "        #discriminator_inference(X_test_abnormal, X_test_normal, epoch)\n",
        "        # plot_model_performance(ch_abnormal, ch_normal, ch_epoch)      \n",
        "         #plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3B269zHqTXN"
      },
      "source": [
        "from IPython.display import Image\n",
        "#Image(filename='image_at_epoch_0060.png') "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXCpnJUAL7tV"
      },
      "source": [
        "type(train_normal_dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o2GT5lm2jzn"
      },
      "source": [
        "# Launch Tensorboard\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir {log_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prIYwgCn2kL8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uU8vILfOqTXO"
      },
      "source": [
        "%%time\n",
        "train(train_normal_dataset, EPOCHS) # train on first 1000 normal samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1KtwqNmqTXO"
      },
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ThrOF6wxqTXO"
      },
      "source": [
        "!ls ../"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5Pol3l_O5ma"
      },
      "source": [
        "## Inferencing Discriminator on X_test ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBsAAeCRZISM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mrlYSlXZIYM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDoDkereTLre"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NN8jt3PLaOyJ"
      },
      "source": [
        "def plot_model_performance(ch_abnormal, ch_normal, ch_epoch):\n",
        "  \"\"\" the function plots abnormal and normal model performance at varying epochs\"\"\"\n",
        "  plt.title('Model performance on test data')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('samples correctly classified')\n",
        "  plt.plot(ch_epoch, ch_abnormal, label='Abnormal')\n",
        "  plt.plot(ch_epoch, ch_normal, label='Normal')\n",
        "  plt.legend()\n",
        "  plt.grid()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7aLC6RPaO2K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liyUN2FRgmuZ"
      },
      "source": [
        "plot_model_performance(chart_abnormal, chart_normal, chart_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm00hQ0Bgm0N"
      },
      "source": [
        " print(chart_abnormal)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLrolxcurDWy"
      },
      "source": [
        "print(chart_normal)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDaAyZI4rDa7"
      },
      "source": [
        "print(chart_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPJErxl2rDen"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5wSfh7zPjLC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4_Umf5CTML6"
      },
      "source": [
        "ch_minority = chart_abnormal\n",
        "ch_majority = chart_normal\n",
        "ch_epoch = chart_epoch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWp-chdekI8O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PGQW8L2nkJBA"
      },
      "source": [
        "# print(f'EPOCH{\"PRECISION\":>12}{\"RECALL\":>7}{\"F1-SCORE\":>9}')\n",
        "print(f'EPOCH{\"PRECISION\":>12}{\"RECALL\":>7}{\"F1-SCORE\":>9}')\n",
        "\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "f1_score_list = []\n",
        "\n",
        "\n",
        "epoch = 0\n",
        "while epoch < len(ch_minority):\n",
        "  true_positive = ch_minority[epoch]\n",
        "  true_negative = ch_majority[epoch]\n",
        "\n",
        "  false_positive = 1000 - true_negative\n",
        "  false_negative = 1000 - true_positive\n",
        "\n",
        "  precision = true_positive / (true_positive + false_positive)\n",
        "  recall = true_positive / (true_positive + false_negative)\n",
        "\n",
        "  f1_score = 2 * ((precision * recall) / (precision + recall))\n",
        "  precision_list.append(round(precision,2))\n",
        "  recall_list.append(round(recall, 2))\n",
        "  f1_score_list.append(round(f1_score, 2))\n",
        "\n",
        "  # print(f'{epoch+1:>5}{precision:.2f>14}{recall:.2f>9}{f1_score:.2f>11}')\n",
        "  print(f'{epoch+1:>5}  {precision:.2f}  {recall:.2f}  {f1_score:.2f}')\n",
        "  epoch += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNpiL4jeFzRJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQTciySjF4rL"
      },
      "source": [
        "# precision_list = []\n",
        "# recall_list = []\n",
        "# f1_score_list = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1HVcUUTJF4vF"
      },
      "source": [
        "print(precision_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_t7jwJHkJFL"
      },
      "source": [
        "print(recall_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX4dDB1zF9IO"
      },
      "source": [
        "print(f1_score_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWmDVs0QGD14"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYUI7yxUGD66"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcXxTJKaF9Nn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9kcbnnJkgby"
      },
      "source": [
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7ZaPObXkJJE"
      },
      "source": [
        "plt.title('Model performance on CIFAR10 test data - imbalance ratio 1:1000')\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('samples correctly classified')\n",
        "plt.plot(ch_epoch, precision_list, label='Precision')\n",
        "plt.plot(ch_epoch, recall_list, label='Recall')\n",
        "plt.plot(ch_epoch, f1_score_list, label='F1-Score')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUi1N6OQkJND"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDqD-7rFkJRB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW_9f-jdkJVH"
      },
      "source": [
        "max(f1_score_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DR4Qq7pRlHqA"
      },
      "source": [
        "print(f'maximum F1-score of {max(f1_score_list)} obtained at epoch {1 + (f1_score_list.index(max(f1_score_list)))}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPY2iv8ylHyL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZJf0dBZkJYg"
      },
      "source": [
        "max(recall_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBDQmsrEl0wc"
      },
      "source": [
        "print(f'maximum recall of {max(recall_list)} obtained at epoch {1 + (recall_list.index(max(recall_list)))}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2hEgUdXkJcq"
      },
      "source": [
        "max(precision_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VYlPMPbKk9o_"
      },
      "source": [
        "print(f'maximum precision of {max(precision_list)} obtained at epoch {1 + (precision_list.index(max(precision_list)))}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqMXnRgJk9uA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiS_QXbWk9zD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNI3A1m4kJg3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAQbmPVTdjKW"
      },
      "source": [
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jr-9_8sRdCEz"
      },
      "source": [
        "def plot_model_performance(ch_minority, ch_majority, ch_epoch):\n",
        "  \"\"\" the function plots abnormal and normal model performance at varying epochs\"\"\"\n",
        "  plt.title('Model performance on test data - imbalance ratio 1:1000')\n",
        "  plt.xlabel('epochs')\n",
        "  plt.ylabel('samples correctly classified')\n",
        "  plt.plot(ch_epoch[:8], ch_minority[:8], label='Minority class')\n",
        "  plt.plot(ch_epoch[:8], ch_majority[:8], label='Majority class')\n",
        "  plt.legend()\n",
        "  plt.grid()\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyZ1Oo7BdCI0"
      },
      "source": [
        "plot_model_performance(ch_minority, ch_majority, ch_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mq_j6acidCSl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HLpXP2FOdCU7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcfqWFdUdCXX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xLYgg7jbdCZ_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKRB4t9TdCcK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9qvKFrrJ5dt"
      },
      "source": [
        "tf.keras.utils.plot_model(discriminator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngpbYYBjJ5i2"
      },
      "source": [
        "tf.keras.utils.plot_model(encoder_generator_network)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VD2mENPKQnR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuIOkrvtLlLW"
      },
      "source": [
        "tf.keras.utils.plot_model(encoder_generator_network, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YX-1RCgNKQrF"
      },
      "source": [
        "tf.keras.utils.plot_model(encoder_generator_network, show_shapes=True, rankdir='LR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BA9QBgg9J5lw"
      },
      "source": [
        "tf.keras.utils.plot_model(encoder_generator_network, rankdir='LR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VK6NpQSzXSkW"
      },
      "source": [
        "tf.keras.utils.plot_model(discriminator, show_shapes=True, rankdir='LR')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbm_dS-IOmXN"
      },
      "source": [
        "---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHTC3TLrOmbd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-kxtMuFVBUWs"
      },
      "source": [
        "## Evaluate best model with best F1-score on test data ##"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_mqrfewqzsp"
      },
      "source": [
        "## Load best saved model ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYxg0Or_q43O"
      },
      "source": [
        "discriminator = tf.keras.models.load_model('/content/gdrive/MyDrive/Anomaly_Detection_in_Images/1617070888/1617070888/1.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYoMu-bOq5Gv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPC0_5cuCDjN"
      },
      "source": [
        "  # To plot model performance at various epochs on abnormal and normal samples\n",
        "  chart_abnormal = []\n",
        "  chart_normal = []\n",
        "  chart_epoch = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9q5ENrLCDjN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncj0h13nCDjN"
      },
      "source": [
        "def discriminator_inference(X_test_abnormal, X_test_normal):\n",
        "  \"\"\"function to calculate discriminator score on text data\"\"\"\n",
        "  print(f'Abnormal-count{\"Normal-count\":>17}')\n",
        "  # print(f'{len(predictions_abnormal_test[0]):>14}{len(decision_test_normal[0]):>14}')\n",
        "\n",
        "  # save model to disk at every epoch\n",
        "  #discriminator.save(f'{time_stamp}/{epoch}.h5')\n",
        "\n",
        "  predictions_abnormal_test = []\n",
        "\n",
        "  # # To plot model performance at various epochs on abnormal and normal samples\n",
        "  # chart_abnormal = []\n",
        "  # chart_normal = []\n",
        "  # chart_epoch = []\n",
        "\n",
        "  for index, data in enumerate(abnormal_data_test[0]):\n",
        "    abn_data_2 = abnormal_data_test[0][index]\n",
        "    predictions_abnormal_test.append(discriminator(abn_data_2))\n",
        "\n",
        "\n",
        "  count_validation_abnormal = 0\n",
        "\n",
        "  for ind, _ in enumerate(predictions_abnormal_test[0]):\n",
        "    if (tf.keras.backend.get_value(predictions_abnormal_test[0][ind])) < 0: # setting a threshold of 0.9\n",
        "      count_validation_abnormal += 1\n",
        "\n",
        "  print('Abnormal correct count: ', count_validation_abnormal)\n",
        "\n",
        "\n",
        "  decision_test_normal = []\n",
        "\n",
        "  for index, img in enumerate(validation_normal_ds):\n",
        "    decision = discriminator(img)\n",
        "    decision_test_normal.append(decision)\n",
        "    #print(decision) # discriminator decision should be close to 1\n",
        "\n",
        "\n",
        "  count_validation_normal = 0\n",
        "\n",
        "  for ind, _ in enumerate(decision_test_normal[0]):\n",
        "    if (tf.keras.backend.get_value(decision_test_normal[0][ind]))[0] >= 0: # setting a threshold of 0.9\n",
        "      #print(decision_test_normal[ind])\n",
        "      count_validation_normal += 1\n",
        "  print('Normal correct count: ', count_validation_normal)\n",
        "\n",
        "\n",
        "  chart_abnormal.append(count_validation_abnormal)\n",
        "  chart_normal.append(count_validation_normal)\n",
        "  #chart_epoch.append(epoch)\n",
        "\n",
        "  # return (chart_abnormal, chart_normal, chart_epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzSaLdeoCDjN"
      },
      "source": [
        "discriminator_inference(X_test_abnormal[500:], X_test_normal[500:]) # evaluate on the remaining test data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHu-IbiMWmTs"
      },
      "source": [
        "ch_minority = chart_abnormal\n",
        "ch_majority = chart_normal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPWaL1JhsDLN"
      },
      "source": [
        "# chart_abnormal[0] = 500\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4Gf95dGsHeE"
      },
      "source": [
        "# chart_normal[0] = 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIN2Jm61p0wc"
      },
      "source": [
        "# print(f'EPOCH{\"PRECISION\":>12}{\"RECALL\":>7}{\"F1-SCORE\":>9}')\n",
        "print(f'EPOCH  {\"PRECISION\"}{\"RECALL\":>7} {\"F1-SCORE\":>9}')\n",
        "\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "f1_score_list = []\n",
        "\n",
        "\n",
        "epoch = 0\n",
        "while epoch < len(ch_minority):\n",
        "    true_positive = ch_minority[epoch]\n",
        "    true_negative = ch_majority[epoch]\n",
        "\n",
        "    false_positive = 500 - true_negative\n",
        "    false_negative = 500 - true_positive\n",
        "\n",
        "    if true_positive == 0:\n",
        "        precision = 0\n",
        "    else:\n",
        "        precision = true_positive / (true_positive + false_positive)\n",
        "        \n",
        "    if true_positive == 0:\n",
        "        recall = 0\n",
        "    else:\n",
        "        recall = true_positive / (true_positive + false_negative)\n",
        "\n",
        "    if precision == 0 and recall == 0:\n",
        "        f1_score = 0\n",
        "    else:\n",
        "        f1_score = 2 * ((precision * recall) / (precision + recall))\n",
        "    \n",
        "    precision_list.append(round(precision,2))\n",
        "    recall_list.append(round(recall, 2))\n",
        "    f1_score_list.append(round(f1_score, 2))\n",
        "\n",
        "    # print(f'{epoch+1:>5}{precision:.2f>14}{recall:.2f>9}{f1_score:.2f>11}')\n",
        "    print(f'{epoch+1:>5}  {precision:.2f}      {recall:.2f}    {f1_score:.2f}')\n",
        "    epoch += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQu7P8nhBkrp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lk-qnWLBkwF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agI5c7_ZBlI6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzfjTgm1bVqg"
      },
      "source": [
        "---"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRDNTaakbVvc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSYtq2PFXSoV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDf0I5qoOv7k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqBOvtScPQqQ"
      },
      "source": [
        "## Discriminator score on abnormal images ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BTsekFBGROhQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9vmVycpPYsc"
      },
      "source": [
        "# predictions_abnormal_test = []\n",
        "\n",
        "# for index, data in enumerate(abnormal_data_test[0]):\n",
        "#   #abn_data_2 = np.expand_dims(abnormal_data_test[0][index], axis=0)\n",
        "#   abn_data_2 = abnormal_data_test[0][index]\n",
        "#   predictions_abnormal_test.append(discriminator(abn_data_2))\n",
        "#   #print(discriminator(abn_data_2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmOPws6WPYwj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cw01xd487YDj"
      },
      "source": [
        "# len(predictions_abnormal_test[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mxYf1k67YHo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdP44vOMPY0x"
      },
      "source": [
        "# count_test_abnormal = 0\n",
        "\n",
        "# for ind, _ in enumerate(predictions_abnormal_test[0]):\n",
        "#   if (tf.keras.backend.get_value(predictions_abnormal_test[0][ind])) < 0: # setting a threshold of 0.9\n",
        "#     count_test_abnormal += 1\n",
        "# print('Abnormal correct count: ', count_test_abnormal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5KjXGkSP_6M"
      },
      "source": [
        "# print(f'{count_test_abnormal} abnormal missed' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH5AsOnu7zK_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA3l6nKJQAIf"
      },
      "source": [
        "# decision_test_normal = []\n",
        "\n",
        "# for index, img in enumerate(test_normal_ds):\n",
        "#   #nor_data_test = np.expand_dims(test_normal_images[int(index)], axis=0)\n",
        "#   #nor_data_test = test_normal_ds\n",
        "\n",
        "#   decision = discriminator(img)\n",
        "#   decision_test_normal.append(decision)\n",
        "#   #print(decision) # discriminator decision should be close to 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4aZ_ISm8Wen"
      },
      "source": [
        "# len(decision_test_normal[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PqOhLt708WkQ"
      },
      "source": [
        "(tf.keras.backend.get_value(decision_test_normal[0][0]))[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PleFlO7-qFq"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pmTD-pfxOwJJ"
      },
      "source": [
        "# count_test_normal = 0\n",
        "\n",
        "# for ind, _ in enumerate(decision_test_normal[0]):\n",
        "#   if (tf.keras.backend.get_value(decision_test_normal[0][ind]))[0] >= 0: # setting a threshold of 0.9\n",
        "#     #print(decision_test_normal[ind])\n",
        "#     count_test_normal += 1\n",
        "# print('Normal correct count: ', count_test_normal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5W91-FB6QO9C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeWZjvSiQPBm"
      },
      "source": [
        "# print(f'{len(decision_test_normal[0]) - count_test_normal} normal missed' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UKStuWMzYl2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMUAgYtSzaaQ"
      },
      "source": [
        "## Save models ##"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnKqi1WzzYpu"
      },
      "source": [
        "#encoder_generator_network.save('encoder_generator.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWzLlJNwzYt-"
      },
      "source": [
        "#discriminator.save('discriminator.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnLpC7j0zYyV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQkgjRQ9OwVx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05BWB3UudiOV"
      },
      "source": [
        "----"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a2ZEnw_4diSL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20JbJG04diVx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_IEuzUGEdiZc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfIY85GGdic_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhZAPDBsdigs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cR_hacOdikI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLeFSa0pdiqp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAG0sVv4qTXO"
      },
      "source": [
        "fig = plt.figure(figsize=(4,4))\n",
        "predictions = checkpoint.generator(x_abnormal, training = False).numpy()\n",
        "for i in range(seed.shape[0]):\n",
        "    plt.subplot(4,4,i+1)\n",
        "    plt.imshow(predictions[i], interpolation=\"nearest\")\n",
        "    plt.axis('off')\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4CFypo3qTXO"
      },
      "source": [
        "noise = tf.random.normal([1,100]) # shape is 1, 100\n",
        "random_face = checkpoint.generator(noise, training = False).numpy()[0]\n",
        "plt.imshow(random_face, interpolation='nearest')\n",
        "plt.axis(\"off\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAv-nJefqTXP"
      },
      "source": [
        "%ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-9ma3QYqTXP"
      },
      "source": [
        "with imageio.get_writer('dcgan.gif', mode='I') as writer:\n",
        "  filenames = glob.glob('image*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  last = -1\n",
        "  for i,filename in enumerate(filenames):\n",
        "    frame = 2*(i**0.5)\n",
        "    if round(frame) > round(last):\n",
        "      last = frame\n",
        "    else:\n",
        "      continue\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)\n",
        "    \n",
        "# A hack to display the GIF inside this notebook\n",
        "os.rename('dcgan.gif', 'dcgan_celebA.gif.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xCmpVSVqTXP"
      },
      "source": [
        "display.Image(filename=\"dcgan_celebA.gif.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBAw_wc4qTXP"
      },
      "source": [
        "%ls training_checkpoints/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4VcjW7TqTXP"
      },
      "source": [
        "%cp image_at_epoch_0001.png play-with-faces/"
      ]
    }
  ]
}